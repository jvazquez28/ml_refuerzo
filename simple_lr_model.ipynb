{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01\n",
      "Training Accuracy: 0.43625\n",
      "Testing Accuracy: 0.435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.43      0.45        60\n",
      "           2       0.35      0.33      0.34        39\n",
      "           3       0.46      0.51      0.48        55\n",
      "           4       0.43      0.43      0.43        46\n",
      "\n",
      "    accuracy                           0.43       200\n",
      "   macro avg       0.43      0.43      0.43       200\n",
      "weighted avg       0.43      0.43      0.43       200\n",
      "\n",
      "Model Parameters: {'C': 100, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "['region', 'tenure', 'age', 'marital', 'address', 'income', 'ed', 'employ', 'retire', 'gender', 'reside']\n",
      "custcat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nTo consume the saved model, follow these steps:\\n\\n1. Load the model using pickle:\\n    with open('best_lr_model.pkl', 'rb') as file:\\n        model_data = pickle.load(file)\\n        model = model_data['model']\\n        scaler = model_data['scaler']\\n        input_schema = model_data['input_schema']\\n        output_schema = model_data['output_schema']\\n\\n2. Preprocess your input data using the same StandardScaler used during training:\\n    scaler = StandardScaler()\\n    scaler.fit(X_train)  # Use the training data to fit the scaler\\n    input_data_scaled = scaler.transform(input_data)\\n\\n3. Make predictions using the loaded model:\\n    predictions = model.predict(input_data_scaled)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load the dataframe\n",
    "df = pd.read_csv('dev/data/teleCust1000t.csv')\n",
    "\n",
    "# Step 2: Preprocess the data using standard scaling\n",
    "X = df.drop('custcat', axis=1)\n",
    "y = df['custcat']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Generate a machine learning model using logistic regression\n",
    "learning_rate = 0.01\n",
    "model = LogisticRegression(C=100, solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(f\"Model Parameters: {model.get_params()}\")\n",
    "print(X_train.columns.tolist())\n",
    "print(y_train.name)\n",
    "\n",
    "# Step 5: Save the best performing model using pickle\n",
    "model_data = {\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'input_schema': X_train.columns.tolist(),\n",
    "    'output_schema': y_train.name\n",
    "}\n",
    "\n",
    "with open('dev/model/best_lr_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model_data, file)\n",
    "\n",
    "# Step 6: Document clearly how to consume the model\n",
    "\"\"\"\n",
    "To consume the saved model, follow these steps:\n",
    "\n",
    "1. Load the model using pickle:\n",
    "    with open('best_lr_model.pkl', 'rb') as file:\n",
    "        model_data = pickle.load(file)\n",
    "        model = model_data['model']\n",
    "        scaler = model_data['scaler']\n",
    "        input_schema = model_data['input_schema']\n",
    "        output_schema = model_data['output_schema']\n",
    "\n",
    "2. Preprocess your input data using the same StandardScaler used during training:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Use the training data to fit the scaler\n",
    "    input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "3. Make predictions using the loaded model:\n",
    "    predictions = model.predict(input_data_scaled)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type before saving: <class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "Model has predict method: True\n",
      "Model attributes: ['C', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__str__', '__subclasshook__', '__weakref__', '_build_request_for_signature', '_check_feature_names', '_check_n_features', '_doc_link_module', '_doc_link_template', '_doc_link_url_param_generator', '_estimator_type', '_get_default_requests', '_get_doc_link', '_get_metadata_request', '_get_param_names', '_get_tags', '_more_tags', '_parameter_constraints', '_predict_proba_lr', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_validate_data', '_validate_params', 'class_weight', 'classes_', 'coef_', 'decision_function', 'densify', 'dual', 'fit', 'fit_intercept', 'get_metadata_routing', 'get_params', 'intercept_', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_features_in_', 'n_iter_', 'n_jobs', 'penalty', 'predict', 'predict_log_proba', 'predict_proba', 'random_state', 'score', 'set_fit_request', 'set_params', 'set_score_request', 'solver', 'sparsify', 'tol', 'verbose', 'warm_start']\n",
      "\n",
      "Loaded model type: <class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "Loaded model has predict: True\n"
     ]
    }
   ],
   "source": [
    "# Verify model has predict method before saving\n",
    "print(f\"Model type before saving: {type(model)}\")\n",
    "print(f\"Model has predict method: {hasattr(model, 'predict')}\")\n",
    "print(f\"Model attributes: {dir(model)}\")\n",
    "\n",
    "# Save model\n",
    "model_path = 'dev/model/best_lr_model.pkl'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Verify saved model\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "    print(f\"\\nLoaded model type: {type(loaded_model)}\")\n",
    "    print(f\"Loaded model has predict: {hasattr(loaded_model, 'predict')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_refuerzo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
